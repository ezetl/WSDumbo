% This file was converted to LaTeX by Writer2LaTeX ver. 1.0.2
% see http://writer2latex.sourceforge.net for more info
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath} % La American Mathematical Society cre\'o este paquete de "Higher Mathematics".
\usepackage{float}
\usepackage{graphicx}
\usepackage{amssymb,amsfonts,textcomp}
\usepackage{color}
\usepackage{mathtools}
\usepackage{courier}
\usepackage[top=2cm,bottom=2cm,left=2cm,right=2cm,nohead,nofoot]{geometry}
\usepackage{array}
\usepackage{hhline}
\usepackage{hyperref}
\usepackage{fixltx2e}
\usepackage{graphicx}

%\sectionfont{\large}

%\usepackage[backend=bibtex]{biblio}
\usepackage[spanish]{babel}
\usepackage{cite} % para contraer referencias
\newcommand\ol[1]{{\setul{-0.9em}{}\ul{#1}}}
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, filecolor=blue, urlcolor=blue}
% Footnote rule
\setlength{\skip\footins}{0.119cm}
\renewcommand\footnoterule{\vspace*{-0.018cm}\setlength\leftskip{0pt}\setlength\rightskip{0pt plus 1fil}\noindent\textcolor{black}{\rule{0.25\columnwidth}{0.018cm}}\vspace*{0.101cm}}
% Pages styles
\makeatletter
\newcommand\ps@Standard{
  \renewcommand\@oddhead{}
  \renewcommand\@evenhead{}
  \renewcommand\@oddfoot{}
  \renewcommand\@evenfoot{}
  \renewcommand\thepage{\arabic{page}}
}
\makeatother
\pagestyle{Standard}

\title{\vspace{-15mm}\fontsize{14pt}{10pt}\selectfont\textbf{Una versión distribuída de discriminación de sentidos de palabras}} % Article title

\author{
\normalsize{Ezequiel Torti L\'opez}\\[2mm] % Your name
\texttt{ezetl91@gmail.com} % Your email address
}

\begin{document}
\maketitle
\abstract{
El propósito de este trabajo fue implementar un algoritmo automático de discriminación de sentidos de palabras sobre una arquitectura distribuída. Para ello se trató de reproducir el algoritmo \emph{Context-Group Discrimination} de Shütze \cite{Sh98}, agregando ciertas restricciones para simplificar su desarrollo. Se propuso con este proyecto no solamente profundizar conocimientos en el área del procesamiento del lenguaje natural, sino también comprender y utilizar un sistema distribuído para el análisis de datos, sentando una base para el procesamiento de corpus de gran tamaño.
}
\section{Introducción}
La discriminación de sentidos agrupa palabras en clases según su significado. Es decir, varias palabras con sentidos similares pertenecen a al misma clase.
En ningún momento de la discriminación de sentidos se requiere etiquetar a los mismos, es decir, definirlos. Simplemente se distinguen, discriminan. Esto hace que no se requieran fuentes de conocimiento externas, como bases de datos o diccionarios, cuyo acceso y procesamiento puede ser costoso y lento. Dado esto, se puede decir que la discriminación de sentidos puede ser implementada de manera automática, pues no requiere de corpus etiquetados o fuentes externas de conocimiento, tareas que requieren mucho trabajo manual.
Si bien puede parecer poco útil tener palabras agrupadas según sentidos pero sin tener los sentidos mismos, en algunas áreas de Information Retrieval tiene aplicaciones, como por ejemplo, medir la similitud entre archivos y hacer ranking de los mismos para presentar los relevantes sobre los no relevantes. También se podría, por ejemplo, utilizar para hacer mejores interfaces a los usuarios: si un usuario ingresa una consulta, se le podría devolver una lista de palabras que tengan un sentido similar a alguna de la consulta que sea ambigua.
\\
En otro plano, en los últimos años comenzó a ser popular un término referido al tratamiento de grandes cantidades de datos: \emph{Big Data}. Con el surgimiento de grandes redes sociales y corporaciones, buscar nuevas formas de procesar las gigantescas cantidades de datos que se generan, y hacerlo lo más rápido posible, es una prioridad. Además, el procesamiento de esos datos es una tarea que en muchas ocasiones está intimamente relacionada con el procesamiento del lenguaje natural.
Algunas soluciones implican utilizar máquinas de última generación y de alto rendimiento, mientras que otras sacan provecho de máquinas existentes, tal vez no tan potentes, pero que en cantidad pueden obtener buenos resultados.
Es por eso que tener un cluster de máquinas puede ser provechoso, si se sabe como explotarlo. En este trabajo se propuso implementar un algoritmo de \emph{Discriminación de Sentidos de Palabras} (\emph{Word Sense Discrimination}) sobre una plataforma distribuída, que sea escalable. Es decir, los objetivos fueron entender y comprender un algoritmo de \emph{Word Sense Discrimination} y además aprender a implementarlo de una forma no trivial sacando provecho de la arquitectura de un cluster de computadoras.

\section{Context-Group Discrimination}
\emph{Context-Group Discrimination} propone clasificar los sentidos de las palabras por las similitudes de los contextos en los que ocurren. Para ello representa palabras, contextos y sentidos en un espacio vectorial (que Shütze denomina \emph{Word Space}). Para construir el contexto en de una palabra, se utilizan las coocurrencias de segundo orden, es decir, se observan las palabras que coocurren con la palabra que coocurre con la palabra ambigua. Está demostrado que se obtienen datos menos dispersos y más precisos con esta técnica.
\subsection{Word Space}
A grandes rasgos, el Word Space comprende Word Vectors, Context Vectors y Sense Vectors.
Un Word Vector contiene información sobre la cantidad de veces que una palabra coocurre con otra. Cada vector representa una palabra y cada \emph{slot} del mismo es una dimension (cada dimension es a su vez una palabra).
Un Context vector es una suma de Word Vectors, o dicho de otra forma, un \emph{centroide}. Cada vez que ocurre una palabra ambigua, se calcula su Context Vector sumando todos los Word Vectors de las palabras con las que coocurre. Esto es lo que anteriormente se mencionó como coocurrencias de segundo orden.
Obtenidos los Context Vectors de todo un corpus, se organizan en clusters y se obtienen sus centroides. Esos centroides se llaman Sense Vectors.
\\
\\
Los resultados obtenidos de este algoritmo pueden depender de la forma en que se representen los vectores en el Word Space. Para este proyecto se decidió que los Word Vectors tengan un tamaño fijo de 2000 dimensiones, mientras que el conjunto de palabras analizadas es de 20000. Para la elección de esas palabras, simpelente se tomaron las más frecuentes del corpus.
Además de esa restricción de tamaño, se aplicó Single Value Descomposition (SVD) a los Word Vectors. La aplicación de SVD ayuda a encontrar los ejes de mayor variación en los vectores y reduce las dimensiones (se utilizó para reducir las dimesiones de 2000 a 100). De esta forma se simplifican las tareas, pues es menos costoso construir clusters de vectores de 100 elementos que de 2000 elementos.
\subsection{Aplicación}
Una vez analizado el corpus de entrenamiento, se hace lo siguiente para discriminar una ocurrencia \em T \em de una palabra ambigua \em V \em:
\begin{enumerate}
\item Obtener el correspondiente Context Vector $\vec c$ usando los Word Vectors de las palabras de su contexto.
\item Obtener los Sense Vectors $\vec S_j$ de \em V \em.
\item Asignar \em T \em al sentido \em j \em cuyo Sense Vector es más cercano a $\vec c$.
\end{enumerate}

\section{Context-Group Discrimination sobre un sistema distribuído}
A la hora de implementar el algoritmo sobre una arquitectura distribuída, se eligió el framework \texttt{Hadoop} \cite{Had}.
Entre otras ventajas que ofrece este framework, hay varias librerías escritas en \texttt{Python} que permiten interacturar con el mismo. En particular, \texttt{Dumbo} \cite{Dum} es una que facilita escribir tareas de \texttt{Hadoop} en \texttt{Python}, de forma que se pudo agilizar el desarrollo y concentrarse en puntos claves del proyecto.
Se instaló \texttt{Hadoop} en una sola máquina y se lo usó en una configuración de nodo simple. Esto se debe a que no se pudo tener acceso a un cluster real con \texttt{Hadoop} y además a que facilitó las tareas en etapas iniciales y de depuración del proyecto.
\subsection{Herramientas y Corpus}
Se utilizó el corpus en español de Wikipedia, el Wiki Corpus. Su tamaño aproximado ronda los 600 MB.
Cabe aclarar que si bien se recomienda recurrir a \texttt{Hadoop} y otras plataformas del estilo cuando se quieren procesar varios Gigas (o Teras) de información, la idea del proyecto no era llegar a esas instancias, por lo que un corpus de un tamaño pequeño como el usado resultó adecuado.
Para el preprocesamiento del corpus se utilizó la librería \texttt{Freeling} \cite{Free}, la mayoría del código se encuentra escrito en \texttt{Python} y para calcular el SVD de los Word Vectors se utilizó un software de terceros, \texttt{redsvd} \cite{Red}.
\subsection{Preprocesamiento de datos}
Se preprocesó el corpus para dejar las palabras en forma de lemas. De esa forma se agilizaron las tareas siguientes, dado que sólo se debía leer de archivos y armar los vectores correspondientes. No contar con la sobrecarga de transformar palabras a lemas en cada ejecución de los pasos fue una gran ayuda, pues tampoco se contaba con un cluster potente sobre el cual correr el algoritmo, y levantar instancias de \texttt{Freeling} y correrlas es muy costoso.
\subsection{Paso 1: Contar palabras}
El primer trabajo sobre \texttt{Hadoop} fue simplemente contar ocurrencias de palabras en el corpus. Esto sirvió para elegir las 20000 más frecuentes y analizar sus características después (notar que las 2000 más frecuentes harán también de dimensiones en los Word Vectors).
\subsection{Paso 2: Contar Coocurrencias}
El segundo trabajo sobre \texttt{Hadoop} tiene como finalidad armar los Word Vectors de cada una de las 20000 palabras más frecuentes, utilizando una ventana de 25 palabras a cada lado. Es decir, cuenta las coocurrencias de cada palabra.
Para ello recorre el corpus una segunda vez, y por cada palabra, chequea que pertenezca al conjunto de las palabras analizables y calcula el vector.
% Si comento cosas de MapReduce, debería agregar lo de:
%\item[Opción 1] por cada coocurrencia, devolver par \em palabra:coocurrencia\em.
%\item[Opción 2] por cada palabra analizada, devolver diccionario de coocurrencias.


\subsection{Paso 3: Calcular SVD}
Una vez obtenida la matriz de coocurrencias, se redujeron las dimensiones aplicando SVD.
Para este paso no se recurrió a \texttt{Hadoop}, se utilizó \texttt{redsvd} \cite{Red}, 
Se redujeron de 2000 a 100, lo cual es un buen número y permite un cálculo más rápido de Context y Sense Vectors.
%Cuando hable de Hadoop, mencionar lo del cuello de botella cuando se reducen las dimensiones.
\subsection{Paso 4: Calcular Context Vectors}
El tercer trabajo en \texttt{Hadoop} consiste en calcular los Context Vectors. Para ello recorre el corpus por tercera vez, analiza las coocurrencias de cada palabra y suma los Word Vectors de las palabras con las que coocurre. De esa forma construye un \emph{centroide}.
Se crea un Context Vector para cada ocurrencia de palabras ambiguas en el corpus, por lo que la cantidad de datos generados en este paso es muy grande y es proporcional al tamaño del corpus. 
\subsection{Paso 5: Construir clusters de Context Vectors}
Se adaptó el algoritmo de K-means clustering para correrlo como un trabajo de \texttt{Hadoop} y explotar el paralelismo ofrecido.
En este paso se calculan los centroides de los clusters que se forman con todos los Context Vectors generados en el paso anterior.
Shütze recomienda utilizar GAAC clustering (\emph{Group-Aglomerative clustering}), que obtiene mejores puntos iniciales que K-means y evita converger a mínimos locales.
Sin embargo se decidió adaptar una implementación de K-means existente al proyecto, obteniendo buenos resultados con una gran cantidad de centroides iniciales.
Finalmente, obtenidos los centroides, se construyeron los clusters según los pasos descriptos en la Sección 2.2.

\section{Resultados}
Se realizó K-means con 12784 semillas iniciales, y se obtuvo una cantidad similar de centroides de clusters.
Una vez clasificados todos los contextos obtenidos del corpus, se procedió a analizar la cualidad de los clusters construidos con diversas métricas. Para ello se tomaron submuestras del total y se construyeron clusters nuevamente, comparando los nuevos resultados con los anteriores de acuerdo a cada métrica.
En el Cuadro 1 se pueden ver los resultados.
% HABLAR DE CADA METRICA POR SEPARADO Y QUE SIGNIFICA CADA UNA.


\section{Conclusiones}
Se logró implementar en este proyecto un algoritmo de discriminación de sentidos de palabras automático, que no requiere de fuentes de conocimientos externas, y si bien no alcanzó un estado de óptimo funcionamiento, se logró una buena performance del mismo, incluso reemplazando GAAC clusering por K-means clustering.\\
A pesar de haber hecho algunas simplificaciones en la adaptación del algoritmo y en las mediciones finales respecto al paper original, la modularización de la implementación deja espacio para posibles futuras mejoras.
Finalmente, se cumplió el objetivo extra de adaptar el algoritmo a una plataforma distribuída. Por el momento su ejecución requiere de un conocimiento amplio del código fuente, pues el desarrollo se centró en la parte funcional, y no tanto en mejorar la interfaz al usuario.

% Bibliografía.
%-----------------------------------------------------------------
\begin{thebibliography}{99}

\bibitem{Sh98} Hinrich Schütze, Automatic Word Sense Discrimination, Computational Linguistics journal - Special issue on word sense disambiguation, Volumen 24, pp. 97-123, 1998.
\bibitem{Had} \url{http://hadoop.apache.org/}
\bibitem{Dum} \url{https://github.com/klbostee/dumbo}
\bibitem{Free} \url{http://nlp.lsi.upc.edu/freeling/}
\bibitem{Red} \url{http://code.google.com/p/redsvd/wiki/English}
\end{thebibliography}
\end{document}
